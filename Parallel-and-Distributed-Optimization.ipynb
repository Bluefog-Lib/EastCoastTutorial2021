{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parallel and Distributed Optimization\n",
    "\n",
    "This notebook provides concise BlueFog demos to concepts or algorithms introduced in the tutorial. \n",
    "\n",
    "\n",
    "### 1.1 Finite-sum optimizaiton example: distributed least square\n",
    "\n",
    "Suppose $m$ computing nodes collaborate to solve the following problem:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{x\\in \\mathbb{R}^d}\\ \\sum_{i=1}^n h_i(x) \\quad \\mbox{where} \\quad h_i(x) = \\frac{1}{2}\\|A_i x - b_i\\|^2 \\hspace{1cm} \\mbox{(Opt-Problem)}\n",
    "\\end{align*}\n",
    "\n",
    "where $h_i(x): \\mathbb{R}^d \\to \\mathbb{R}$ is a local cost function held by node $i$ and $\\{A_i, b_i\\}$ are local data. Each node $i$ can evaluate its own data and gradient, but it has to communicate to achieve information from the other node. We let $x^\\star$ denote the global solution to the above problem\n",
    "\n",
    "### 1.2 All-reduce\n",
    "\n",
    "BlueFog supports the Ring-Allreduce operation as follows: \n",
    "```\n",
    "x_bar = bf.allreduce(x_local)\n",
    "```\n",
    "\n",
    "In the following code, we activate 8 computing nodes (CPUs) and label them as $0,1,\\cdots,7$. We target to let all nodes collaborate to compute the global average of their labels $(\\sum_{i=0}^7 i)/8 = 3.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting allreduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile allreduce.py\n",
    " \n",
    "import bluefog.torch as bf\n",
    "import torch\n",
    "\n",
    "bf.init()\n",
    "x_local = torch.ones(1) * bf.rank()\n",
    "x_bar = bf.allreduce(x_local)\n",
    "print('Node {} achieved the global average {}'.format(bf.rank(), x_bar[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 0 achieved the global average 3.5\r\n",
      "Node 2 achieved the global average 3.5\r\n",
      "Node 4 achieved the global average 3.5\r\n",
      "Node 5 achieved the global average 3.5\r\n",
      "Node 6 achieved the global average 3.5\r\n",
      "Node 7 achieved the global average 3.5\r\n",
      "Node 1 achieved the global average 3.5\r\n",
      "Node 3 achieved the global average 3.5\r\n"
     ]
    }
   ],
   "source": [
    "! bfrun -np 8 python allreduce.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Distributed Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributed gradient descent is $x^{k+1} = x^k - \\frac{\\alpha}{n}\\sum_{i=1}^n \\nabla h_i(x^k)$. This process can be implemented as\n",
    "\n",
    "```python\n",
    "# core distributed gradient descent snippet \n",
    "grad = A.T.mm(A.mm(x) - b)\n",
    "next_x = x - alpha * bf.allreduce(grad)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DistributedGD.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile DistributedGD.py\n",
    " \n",
    "import bluefog.torch as bf\n",
    "import torch\n",
    "\n",
    "bf.init()\n",
    "\n",
    "def generate_data(m, d):\n",
    "    A = torch.randn(m, d).to(torch.double)\n",
    "    ns = 0.1*torch.randn(m, 1).to(torch.double)\n",
    "    x_o = torch.rand(d,1).to(torch.double)\n",
    "    b = A.mm(x_o) + ns\n",
    "    \n",
    "    return A, b\n",
    "\n",
    "def check_opt_cond(x, A, b):\n",
    "    \n",
    "    grad_local = A.t().mm(A.mm(x) - b)\n",
    "    grad = bf.allreduce(grad_local, name='gradient')  # global gradient\n",
    "    \n",
    "    # the norm of global gradient is expected to be 0 (optimality condition)\n",
    "    global_grad_norm = torch.norm(grad, p=2)\n",
    "    print(\"[Distributed Grad Descent] Rank {}: global gradient norm: {}\".format(bf.rank(), global_grad_norm))\n",
    "        \n",
    "    return\n",
    "    \n",
    "\n",
    "def distributed_grad_descent(A, b, maxite=5000, alpha=1e-1):\n",
    "\n",
    "    m, d = A.shape\n",
    "    \n",
    "    x_opt = torch.zeros(d, 1, dtype=torch.double)\n",
    "\n",
    "    for _ in range(maxite):\n",
    "        # calculate local gradient \n",
    "        grad_local = A.t().mm(A.mm(x_opt) - b)\n",
    "        \n",
    "        # global gradient\n",
    "        grad = bf.allreduce(grad_local, name='gradient')\n",
    "\n",
    "        # distributed gradient descent\n",
    "        x_opt = x_opt - alpha*grad\n",
    "\n",
    "    check_opt_cond(x_opt, A, b)\n",
    "    \n",
    "    return x_opt\n",
    "\n",
    "m, d = 20, 5 # dimension of A\n",
    "A, b = generate_data(m, d)\n",
    "x_opt = distributed_grad_descent(A, b, maxite=200, alpha=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x^k\\to x^\\star$, it holds that $\\sum_{i=1}^m \\nabla h_i(x^k) \\to 0$. We can use $\\|\\sum_{i=1}^m \\nabla h_i(x)\\|$ as a metric to gauge whether distributed gradient descent converge or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Distributed Grad Descent] Rank 1: global gradient norm: 1.003372005090602e-13\r\n",
      "[Distributed Grad Descent] Rank 2: global gradient norm: 1.003372005090602e-13\r\n",
      "[Distributed Grad Descent] Rank 5: global gradient norm: 1.003372005090602e-13\r\n",
      "[Distributed Grad Descent] Rank 7: global gradient norm: 1.003372005090602e-13\r\n",
      "[Distributed Grad Descent] Rank 0: global gradient norm: 1.003372005090602e-13\r\n",
      "[Distributed Grad Descent] Rank 3: global gradient norm: 1.003372005090602e-13\r\n",
      "[Distributed Grad Descent] Rank 4: global gradient norm: 1.003372005090602e-13\r\n",
      "[Distributed Grad Descent] Rank 6: global gradient norm: 1.003372005090602e-13\r\n"
     ]
    }
   ],
   "source": [
    "! bfrun -np 8 python DistributedGD.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Primal Decomposition\n",
    "\n",
    "We consider the following linearly-constrained resource sharing problem:\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\{x_i\\}, y}&\\quad \\|y\\|_1 + \\frac{1}{2}\\sum_{i=1}^n \\|A_i x_i - b_i\\|^2 \\\\\n",
    "\\mathrm{s.t.} &\\quad\\sum_{i=1}^n B_i x_i = y\n",
    "\\end{align*}\n",
    "\n",
    "where $x_i \\in \\mathbb{R}^d$, $A_i \\in \\mathbb{R}^{m\\times d}$, and $B_i \\in \\mathbb{R}^{d\\times d}$. For simplicity, we assume each $B_i$ is a invertible matrix. Each node $i$ has local data $\\{A_i, B_i, b_i\\}$. To solve the above problem in a distributed manner, we introduce $y_i = B_i x_i$ so that $y=\\sum_{i=1}^n y_i$. Since $B_i$ is invertible, we have $x_i = B_i^{-1} y_i$. Substituting these facts into the above problem, we achieve\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\{y_i\\}}&\\quad \\|y_1 + \\cdots + y_n\\|_1 + \\frac{1}{2}\\sum_{i=1}^n \\|A_i B_i^{-1} y_i - b_i\\|^2 \n",
    "\\end{align*}\n",
    "\n",
    "which can be solved in the following distributed manner. For notation simplicity, we let $C_i = A_i B_i^{-1}$ and $g(y_1,\\cdots, y_n) = \\|y_1 + \\cdots + y_n\\|_1$. Following proximal gradient descent, we have \n",
    "\n",
    "\\begin{align*}\n",
    "z_i^{k+1} &= y_i^k - \\alpha C_i^T(C_i y_i^k - b_i), \\quad \\forall\\ i=1,\\cdots, n\\\\\n",
    "y_i^{k+1} &= [\\mathrm{Prox}_{\\alpha g}(z_1^{k+1},\\cdots, z_n^{k+1})]_{i} =z_i^{k+1} - \\alpha v^{k+1}, \\quad \\forall\\ i=1,\\cdots, n \\\\\n",
    "x_i^{k+1} &= B_i^{-1} y_i^{k+1}, \\quad \\forall\\ i=1,\\cdots, n\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "$$v^{k+1} = \\frac{1}{n\\alpha^2}\\big(\\alpha(z^{k+1}_1 + \\cdots + z^{k+1}_n) - \\mathrm{Prox}_{n \\alpha^2 \\|\\cdot\\|_1}(\\alpha z^{k+1}_1 + \\cdots + \\alpha z^{k+1}_n)\\big) \\quad \\mbox{(Need Allreduce Communication)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PrimalDecompose.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PrimalDecompose.py\n",
    " \n",
    "import bluefog.torch as bf\n",
    "import torch\n",
    "\n",
    "bf.init()\n",
    "\n",
    "def generate_data(m, d):\n",
    "    A = torch.randn(m, d).to(torch.double)\n",
    "    B_inv = torch.randn(d, d).to(torch.double)\n",
    "    ns = 0.1*torch.randn(m, 1).to(torch.double)\n",
    "    x_o = torch.rand(d,1).to(torch.double)\n",
    "    b = A.mm(x_o) + ns\n",
    "    \n",
    "    return A, B_inv, b\n",
    "\n",
    "def soft_threshold(x, kappa):\n",
    "    \n",
    "    zeros = torch.zeros(d,1).to(torch.double)\n",
    "    x = torch.max(x - kappa, zeros)\n",
    "    x = torch.min(x + kappa, zeros)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def check_opt_cond(y, A, B_inv, b, alpha):\n",
    "    \n",
    "    m, d = A.shape\n",
    "    n = bf.size()\n",
    "    C = A.mm(B_inv)\n",
    "    \n",
    "    # update z\n",
    "    grad_local = C.t().mm(C.mm(y) - b)\n",
    "    z = y - alpha*grad_local\n",
    "    z_bar = bf.allreduce(z)\n",
    "    z_sum = z_bar * n\n",
    "\n",
    "    # update v\n",
    "    v = (1/(n*alpha*alpha)) * (alpha*z_sum - soft_threshold(alpha*z_sum, n*alpha*alpha))\n",
    "\n",
    "    # update y\n",
    "    y_next = z - alpha*v\n",
    "    \n",
    "    # the norm of global gradient is expected to be 0 (optimality condition)\n",
    "    global_grad_norm = torch.norm((y - y_next), p=2)\n",
    "    print(\"[Primal Decomposition] Rank {}: optimality metric norm: {}\".format(bf.rank(), global_grad_norm))\n",
    "        \n",
    "    return\n",
    "\n",
    "def primal_decomposition(A, B_inv, b, maxite=5000, alpha=1e-1):\n",
    "\n",
    "    m, d = A.shape\n",
    "    n = bf.size()\n",
    "    C = A.mm(B_inv)\n",
    "    \n",
    "    y = torch.zeros(d, 1, dtype=torch.double)\n",
    "\n",
    "    for _ in range(maxite):\n",
    "        # update z\n",
    "        grad_local = C.t().mm(C.mm(y) - b)\n",
    "        z = y - alpha*grad_local\n",
    "        z_bar = bf.allreduce(z)\n",
    "        z_sum = z_bar * n\n",
    "        \n",
    "        # update v\n",
    "        v = (1/(n*alpha*alpha)) * (alpha*z_sum - soft_threshold(alpha*z_sum, n*alpha*alpha))\n",
    "        \n",
    "        # update y\n",
    "        y = z - alpha*v\n",
    "\n",
    "        # update x\n",
    "        x = B_inv.mm(y)\n",
    "\n",
    "    check_opt_cond(y, A, B_inv, b, alpha)\n",
    "    \n",
    "    return y\n",
    "\n",
    "m, d = 20, 5 # dimension of A\n",
    "A, B_inv, b = generate_data(m, d)\n",
    "y = primal_decomposition(A, B_inv, b, maxite=10000, alpha=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define $G^k_i = \\frac{1}{\\alpha}\\big(y_i^k - [\\mathrm{Prox}_{\\alpha g}(z_1^{k+1},\\cdots, z_n^{k+1})]_i\\big)$ and use $\\|G^k_i\\|$ as the metric to evaluate the optimality of $y_i^k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Primal Decomposition] Rank 0: optimality metric norm: 8.528189199938493e-12\r\n",
      "[Primal Decomposition] Rank 1: optimality metric norm: 1.7176591460971584e-10\r\n",
      "[Primal Decomposition] Rank 2: optimality metric norm: 4.8323149942724454e-11\r\n",
      "[Primal Decomposition] Rank 3: optimality metric norm: 4.151387808746409e-12\r\n",
      "[Primal Decomposition] Rank 5: optimality metric norm: 9.245811095404164e-12\r\n",
      "[Primal Decomposition] Rank 7: optimality metric norm: 1.534382178553307e-10\r\n",
      "[Primal Decomposition] Rank 6: optimality metric norm: 3.4050679775704995e-12\r\n",
      "[Primal Decomposition] Rank 4: optimality metric norm: 9.703220480870633e-11\r\n"
     ]
    }
   ],
   "source": [
    "! bfrun -np 8 python PrimalDecompose.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Distributed ADMM\n",
    "\n",
    "\\[Bicheng will add this section\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
